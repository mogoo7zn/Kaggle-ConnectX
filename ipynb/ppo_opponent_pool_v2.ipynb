{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO Training v2 - Diverse Opponent Pool\n",
        "\n",
        "> 本 Notebook 为 v2 版本，在不修改原始 `ppo_frozen_opponents.ipynb` 的前提下，增加“多样化对手池”机制，提升 PPO 的泛化与鲁棒性。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境准备\n",
        "- 克隆仓库，进入根目录\n",
        "- 安装必要依赖（kaggle 环境通常自带 torch，如缺少可按需安装）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mogoo7zn/Kaggle-ConnectX.git\n",
        "%cd Kaggle-ConnectX\n",
        "!pip install -q kaggle-environments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 导入与路径设置\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os, sys, random, importlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 确定仓库根\n",
        "candidates = [Path('/kaggle/working/Kaggle-ConnectX'), Path.cwd(), Path.cwd().parent]\n",
        "repo_root = next((c for c in candidates if (c/'agents').exists()), Path('.').resolve())\n",
        "os.chdir(repo_root)\n",
        "sys.path.insert(0, str(repo_root))\n",
        "importlib.invalidate_caches()\n",
        "print('Repo root:', repo_root)\n",
        "\n",
        "from agents.ppo.ppo_agent import PPOAgent\n",
        "from agents.ppo.ppo_config import ppo_config\n",
        "from agents.base.utils import get_valid_moves, get_negamax_move, encode_state, make_move, is_terminal\n",
        "from agents.dqn.dqn_agent import DQNAgent\n",
        "\n",
        "# AlphaZero loader\n",
        "try:\n",
        "    from agents.alphazero.az_model import create_alphazero_model\n",
        "except Exception:\n",
        "    create_alphazero_model = None\n",
        "    print('AlphaZero loader unavailable')\n",
        "\n",
        "DEVICE = ppo_config.DEVICE\n",
        "print('Using device:', DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 加载冻结对手（DQN / AlphaZero，可选）\n",
        "支持从已下载的 checkpoint 加载。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ckpt_dir = Path('/kaggle/working/checkpoints')\n",
        "ckpt_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# 在此填入已下载权重路径（若已在工作目录，可直接指定文件名）\n",
        "CKPT_DQN = ckpt_dir / 'dqn_frozen.pth'\n",
        "CKPT_AZ  = ckpt_dir / 'alphazero_frozen.pth'\n",
        "\n",
        "frozen_dqn = None\n",
        "if CKPT_DQN.exists():\n",
        "    frozen_dqn = DQNAgent()\n",
        "    frozen_dqn.load_model(str(CKPT_DQN))\n",
        "    print('Loaded frozen DQN:', CKPT_DQN)\n",
        "else:\n",
        "    print('No DQN checkpoint found:', CKPT_DQN)\n",
        "\n",
        "frozen_az = None\n",
        "if CKPT_AZ.exists() and create_alphazero_model:\n",
        "    try:\n",
        "        frozen_az = create_alphazero_model('full')  # 如权重为轻量版改 'light'\n",
        "        state = torch.load(CKPT_AZ, map_location=DEVICE)\n",
        "        if isinstance(state, dict) and 'model_state_dict' in state:\n",
        "            state = state['model_state_dict']\n",
        "        frozen_az.load_state_dict(state, strict=False)\n",
        "        frozen_az.eval()\n",
        "        print('Loaded frozen AlphaZero:', CKPT_AZ)\n",
        "    except Exception as e:\n",
        "        print('Failed to load AlphaZero:', e)\n",
        "        frozen_az = None\n",
        "else:\n",
        "    print('No AlphaZero checkpoint found or loader unavailable:', CKPT_AZ)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 对手定义（模块化）\n",
        "包含随机、negamax、冻结 DQN、冻结 AZ、当前 PPO、自身历史版本、以及 PPO 自博弈低概率采样。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def random_policy(board, mark):\n",
        "    moves = get_valid_moves(board)\n",
        "    return random.choice(moves) if moves else 0\n",
        "\n",
        "def negamax_policy(board, mark, depth=4):\n",
        "    return get_negamax_move(board, mark, depth=depth)\n",
        "\n",
        "def frozen_dqn_policy(board, mark):\n",
        "    assert frozen_dqn is not None, \"frozen_dqn not loaded\"\n",
        "    return frozen_dqn.select_action(board, mark, epsilon=0.0)\n",
        "\n",
        "def frozen_alphazero_policy(board, mark):\n",
        "    assert frozen_az is not None, \"frozen_az not loaded\"\n",
        "    state = encode_state(board, mark)\n",
        "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = frozen_az(state_t)\n",
        "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    valid = get_valid_moves(board)\n",
        "    masked = np.full_like(probs, -1e9)\n",
        "    masked[valid] = probs[valid]\n",
        "    return int(masked.argmax())\n",
        "\n",
        "# 当前 PPO 策略（训练中的 agent，会更新）\n",
        "def ppo_policy_live(agent):\n",
        "    def _policy(board, mark):\n",
        "        state = encode_state(board, mark)\n",
        "        state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            logits, _ = agent.model(state_t)\n",
        "            valid = get_valid_moves(board)\n",
        "            mask = torch.full_like(logits, float('-inf'))\n",
        "            mask[0, valid] = 0\n",
        "            logits = logits + mask\n",
        "            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "        return int(probs.argmax())\n",
        "    return _policy\n",
        "\n",
        "# 历史 PPO 检查点策略（冻结）\n",
        "def ppo_policy_from_state_dict(state_dict):\n",
        "    from agents.ppo.ppo_model import make_model\n",
        "    m = make_model()\n",
        "    m.load_state_dict(state_dict)\n",
        "    m.eval()\n",
        "    def _policy(board, mark):\n",
        "        state = encode_state(board, mark)\n",
        "        state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            logits, _ = m(state_t)\n",
        "            valid = get_valid_moves(board)\n",
        "            mask = torch.full_like(logits, float('-inf'))\n",
        "            mask[0, valid] = 0\n",
        "            logits = logits + mask\n",
        "            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "        return int(probs.argmax())\n",
        "    return _policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 对手池组件（OpponentPool）\n",
        "- 支持按概率采样对手\n",
        "- 支持定期加入 PPO 历史 checkpoint（冻结）\n",
        "- 仅 PPO 主体更新，其他对手冻结\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "class OpponentPool:\n",
        "    def __init__(self, base_prob=None, snapshot_limit=5):\n",
        "        # base_prob 定义每类对手的采样概率\n",
        "        self.base_prob = base_prob or {\n",
        "            'random': 0.10,\n",
        "            'negamax': 0.20,\n",
        "            'frozen_dqn': 0.20 if frozen_dqn else 0.0,\n",
        "            'frozen_az': 0.20 if frozen_az else 0.0,\n",
        "            'ppo_self': 0.20,      # 当前 PPO（自博弈）\n",
        "            'ppo_history': 0.10,   # 历史快照\n",
        "        }\n",
        "        self.snapshots = []  # 存储 state_dict\n",
        "        self.snapshot_limit = snapshot_limit\n",
        "        self._rebuild()\n",
        "\n",
        "    def _rebuild(self):\n",
        "        # 构建可用对手列表\n",
        "        self.entries = []\n",
        "        def add(name, fn):\n",
        "            p = self.base_prob.get(name, 0.0)\n",
        "            if p > 0:\n",
        "                self.entries.append((p, name, fn))\n",
        "        add('random', random_policy)\n",
        "        add('negamax', lambda b,m: negamax_policy(b,m,depth=4))\n",
        "        if frozen_dqn:\n",
        "            add('frozen_dqn', frozen_dqn_policy)\n",
        "        if frozen_az:\n",
        "            add('frozen_az', frozen_alphazero_policy)\n",
        "        add('ppo_self', ppo_policy_live(agent))\n",
        "        # 历史快照\n",
        "        for i, sd in enumerate(self.snapshots):\n",
        "            add(f'ppo_hist_{i}', ppo_policy_from_state_dict(sd))\n",
        "        # 归一化权重\n",
        "        total = sum(p for p,_,_ in self.entries)\n",
        "        self.entries = [(p/total, n, f) for p,n,f in self.entries if total > 0]\n",
        "\n",
        "    def add_snapshot(self, state_dict):\n",
        "        self.snapshots.append(state_dict)\n",
        "        if len(self.snapshots) > self.snapshot_limit:\n",
        "            self.snapshots.pop(0)\n",
        "        self._rebuild()\n",
        "\n",
        "    def sample(self):\n",
        "        if not self.entries:\n",
        "            return random_policy\n",
        "        probs = [p for p,_,_ in self.entries]\n",
        "        choices = list(range(len(self.entries)))\n",
        "        idx = np.random.choice(choices, p=probs)\n",
        "        return self.entries[idx][2], self.entries[idx][1]\n",
        "\n",
        "# 初始化对手池\n",
        "def build_pool():\n",
        "    pool = OpponentPool()\n",
        "    print('Opponent entries:')\n",
        "    for p, n, _ in pool.entries:\n",
        "        print(f'  {n}: {p:.2f}')\n",
        "    return pool\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 训练循环（加入对手池采样 + 历史快照）\n",
        "- 每个 update 随机从对手池按概率采样\n",
        "- 每隔 SNAPSHOT_INTERVAL 将当前 PPO 参数加入历史快照（冻结）\n",
        "- 只有 PPO 主体更新，其他对手冻结\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "agent = PPOAgent()\n",
        "opponent_pool = None\n",
        "\n",
        "TOTAL_UPDATES = 200    # 可调大\n",
        "ROLLOUT_STEPS = 512    # 可调大\n",
        "LOG_INTERVAL = 20\n",
        "SNAPSHOT_INTERVAL = 50  # 每隔多少次更新保存一次 PPO 快照\n",
        "\n",
        "reward_log = []\n",
        "\n",
        "for update in range(1, TOTAL_UPDATES + 1):\n",
        "    if opponent_pool is None:\n",
        "        opponent_pool = build_pool()\n",
        "    opp_fn, opp_name = opponent_pool.sample()\n",
        "    batch = agent.generate_rollout(opp_fn, ROLLOUT_STEPS)\n",
        "    loss = agent.update(batch)\n",
        "    reward_log.append(batch.returns.mean().item())\n",
        "\n",
        "    # 定期保存 PPO 快照加入对手池\n",
        "    if update % SNAPSHOT_INTERVAL == 0:\n",
        "        snapshot_sd = agent.model.state_dict()\n",
        "        opponent_pool.add_snapshot(snapshot_sd)\n",
        "        print(f\"[Snapshot] added at update {update}, snapshots={len(opponent_pool.snapshots)}\")\n",
        "\n",
        "    if update % LOG_INTERVAL == 0:\n",
        "        avg_ret = float(np.mean(reward_log[-LOG_INTERVAL:]))\n",
        "        print(f\"Update {update}/{TOTAL_UPDATES} | opp={opp_name} | loss {loss:.3f} | avg_ret {avg_ret:.3f}\")\n",
        "\n",
        "# 保存模型\n",
        "ppo_path = Path('/kaggle/working/ppo_pool_v2.pth')\n",
        "torch.save(agent.model.state_dict(), ppo_path)\n",
        "print(\"Saved\", ppo_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 简单评估（交替先手 + 贪心）\n",
        "- 交替先后手，避免先手偏置\n",
        "- 贪心落子，减少随机波动\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def play_one(policy_fn, opp_fn, games=20):\n",
        "    wins = 0\n",
        "    for g in range(games):\n",
        "        board = [0]*(ppo_config.ROWS*ppo_config.COLUMNS)\n",
        "        ppo_mark = 1 if g % 2 == 0 else 2\n",
        "        opp_mark = 3 - ppo_mark\n",
        "        current = 1\n",
        "        while True:\n",
        "            if current == ppo_mark:\n",
        "                action = policy_fn(board, current)\n",
        "            else:\n",
        "                action = opp_fn(board, current)\n",
        "            board = make_move(board, action, current)\n",
        "            done, winner = is_terminal(board)\n",
        "            if done:\n",
        "                if winner == ppo_mark:\n",
        "                    wins += 1\n",
        "                break\n",
        "            current = 3 - current\n",
        "    return wins / games\n",
        "\n",
        "def ppo_policy_greedy(board, mark):\n",
        "    state = encode_state(board, mark)\n",
        "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = agent.model(state_t)\n",
        "        valid = get_valid_moves(board)\n",
        "        mask = torch.full_like(logits, float('-inf'))\n",
        "        mask[0, valid] = 0\n",
        "        logits = logits + mask\n",
        "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    return int(probs.argmax())\n",
        "\n",
        "print(\"PPO vs random:\", play_one(ppo_policy_greedy, random_policy, games=20))\n",
        "print(\"PPO vs negamax:\", play_one(ppo_policy_greedy, lambda b,m: negamax_policy(b,m,depth=4), games=20))\n",
        "if frozen_dqn:\n",
        "    print(\"PPO vs frozen DQN:\", play_one(ppo_policy_greedy, frozen_dqn_policy, games=20))\n",
        "if frozen_az:\n",
        "    print(\"PPO vs frozen AlphaZero:\", play_one(ppo_policy_greedy, frozen_alphazero_policy, games=20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 打包提交（与 v1 相同，只是文件名不同）\n",
        "- 输出：`/kaggle/working/ppo_pool_v2.pth` + 生成 `agent.py` + `ppo_submission_v2.zip`\n",
        "- 仍可按原流程下载 zip 上传竞赛\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from textwrap import dedent\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "submit_dir = Path('/kaggle/working/ppo_submit_v2')\n",
        "submit_dir.mkdir(exist_ok=True, parents=True)\n",
        "model_out = submit_dir / 'ppo_pool_v2.pth'\n",
        "shutil.copy(ppo_path, model_out)\n",
        "\n",
        "agent_code = dedent(f\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "ROWS, COLS, INAROW = 6, 7, 4\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.fc = nn.Linear(ROWS * COLS * 128, 256)\n",
        "        self.policy = nn.Linear(256, COLS)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc(x))\n",
        "        logits = self.policy(x)\n",
        "        return logits\n",
        "\n",
        "def encode_state(board, mark):\n",
        "    board_2d = np.array(board).reshape(ROWS, COLS)\n",
        "    opp = 3 - mark\n",
        "    player = (board_2d == mark).astype(np.float32)\n",
        "    opponent = (board_2d == opp).astype(np.float32)\n",
        "    valid = np.zeros((ROWS, COLS), dtype=np.float32)\n",
        "    for c in range(COLS):\n",
        "        if board_2d[0, c] == 0:\n",
        "            valid[:, c] = 1.0\n",
        "    state = np.stack([player, opponent, valid], axis=0)\n",
        "    return state\n",
        "\n",
        "def get_valid_moves(board):\n",
        "    return [c for c in range(COLS) if board[c] == 0]\n",
        "\n",
        "def agent(obs, config):\n",
        "    global _model\n",
        "    if '_model' not in globals():\n",
        "        _model = ActorCritic().to(DEVICE)\n",
        "        sd = torch.load('ppo_pool_v2.pth', map_location=DEVICE)\n",
        "        _model.load_state_dict(sd)\n",
        "        _model.eval()\n",
        "    board = obs.board\n",
        "    mark = obs.mark\n",
        "    valid = get_valid_moves(board)\n",
        "    if not valid:\n",
        "        return 0\n",
        "    state = torch.from_numpy(encode_state(board, mark)).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits = _model(state)\n",
        "        mask = torch.full_like(logits, float('-inf'))\n",
        "        mask[0, valid] = 0\n",
        "        logits = logits + mask\n",
        "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    return int(probs.argmax())\n",
        "\"\"\")\n",
        "\n",
        "with open(submit_dir / 'agent.py', 'w') as f:\n",
        "    f.write(agent_code)\n",
        "\n",
        "zip_path = Path('/kaggle/working/ppo_submission_v2.zip')\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    zf.write(submit_dir / 'agent.py', arcname='agent.py')\n",
        "    zf.write(submit_dir / 'ppo_pool_v2.pth', arcname='ppo_pool_v2.pth')\n",
        "\n",
        "print('Submission zip created:', zip_path)\n",
        "!ls -lh /kaggle/working/ppo_submission_v2.zip\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
