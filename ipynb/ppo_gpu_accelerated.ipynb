{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training with GPU Acceleration\n",
    "\n",
    "使用向量化环境进行批量 rollout，大幅提升 GPU 利用率和训练速度。\n",
    "支持 A100 等高端 GPU 高效训练 ConnectX 智能体。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 克隆代码仓库（云端首次运行需要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/mogoo7zn/Kaggle-ConnectX.git\n",
    "%cd Kaggle-ConnectX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 环境与依赖\n",
    "- 需 GPU 环境（Kaggle GPU / Colab Pro / 本地 GPU）\n",
    "- 安装最小依赖：`kaggle-environments`（对战模拟）、`huggingface_hub`（可选上传）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle-environments huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "\n",
    "# 让项目包可被导入：优先常见 Kaggle 路径，否则尝试当前/父目录\n",
    "candidates = [\n",
    "    Path('/kaggle/working/Kaggle-ConnectX'),  # git clone 默认位置\n",
    "    Path('/kaggle/working'),                  # 若 notebook 已位于仓库内，这里无 agents 则会跳过\n",
    "    Path.cwd(),\n",
    "    Path.cwd().parent,\n",
    "]\n",
    "repo_root = None\n",
    "for c in candidates:\n",
    "    if (c / 'agents').exists():\n",
    "        repo_root = c\n",
    "        break\n",
    "if repo_root is None:\n",
    "    repo_root = Path('.').resolve()\n",
    "\n",
    "os.chdir(repo_root)\n",
    "sys.path.insert(0, str(repo_root))\n",
    "importlib.invalidate_caches()\n",
    "print(\"Repo root:\", repo_root)\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"agents exists:\", (repo_root / 'agents').exists())\n",
    "\n",
    "from agents.ppo.ppo_agent import PPOAgent\n",
    "from agents.ppo.ppo_config import ppo_config\n",
    "from agents.base.utils import get_valid_moves, get_negamax_move, encode_state, make_move, is_terminal\n",
    "from agents.dqn.dqn_agent import DQNAgent\n",
    "\n",
    "# 可选：AlphaZero 载入（需要与你队友的权重匹配）\n",
    "try:\n",
    "    from agents.alphazero.az_model import create_alphazero_model\n",
    "except Exception:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    create_alphazero_model = None\n",
    "\n",
    "DEVICE = ppo_config.DEVICE\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 模型架构优化\n",
    "使用更大的网络容量以充分利用 GPU 计算能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 可在 notebook 内直接定义/替换模型，避免每次修改后重新 clone\n",
    "# 调整下方通道/隐藏层尺寸即可放大模型规模\n",
    "import types\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from agents.ppo import ppo_model\n",
    "\n",
    "class ActorCriticGPU(nn.Module):\n",
    "    \"\"\"\n",
    "    针对 GPU 优化的更大网络架构\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 大幅增加通道数以充分利用 GPU\n",
    "        self.conv1 = nn.Conv2d(3, 256, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.conv2 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        conv_out = ppo_config.ROWS * ppo_config.COLUMNS * 512\n",
    "        self.fc1 = nn.Linear(conv_out, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.policy = nn.Linear(1024, ppo_config.COLUMNS)\n",
    "        self.value = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.policy(x)\n",
    "        value = self.value(x)\n",
    "        return logits, value\n",
    "\n",
    "# 覆盖原有工厂方法\n",
    "ppo_model.ActorCritic = ActorCriticGPU\n",
    "ppo_model.make_model = lambda: ActorCriticGPU().to(ppo_config.DEVICE)\n",
    "\n",
    "print(\"Using GPU-optimized ActorCritic with large capacity:\", ActorCriticGPU())\n",
    "print(f\"Model parameters: {sum(p.numel() for p in ActorCriticGPU().parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 下载队友的已训练模型\n",
    "作为冻结对手加入训练池"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# 直接填入公开仓库的 resolve 链接\n",
    "DQN_URL = \"https://huggingface.co/mogoo7zn/Kaggle-ConnectX/resolve/main/DQN-base.pth\"\n",
    "AZ_URL  = \"https://huggingface.co/mogoo7zn/Kaggle-ConnectX/resolve/main/alpha-zero-medium.pth\"\n",
    "# 如果你想用 high 版，就把上面这一行改成 alpha-zero-high.pth\n",
    "\n",
    "ckpt_dir = Path('/kaggle/working/checkpoints')\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download(url, out_path):\n",
    "    if not url:\n",
    "        print(f\"[skip] empty url for {out_path.name}\")\n",
    "        return None\n",
    "    print(f\"Downloading {url} -> {out_path}\")\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    with open(out_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return out_path\n",
    "\n",
    "ckpt_dqn = download(DQN_URL, ckpt_dir/'dqn_frozen.pth')\n",
    "ckpt_az  = download(AZ_URL,  ckpt_dir/'alphazero_frozen.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加载冻结对手\n",
    "DQN 和 AlphaZero 作为固定难度的对手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "frozen_dqn = None\n",
    "if ckpt_dqn and ckpt_dqn.exists():\n",
    "    frozen_dqn = DQNAgent()\n",
    "    frozen_dqn.load_model(str(ckpt_dqn))\n",
    "    # 确保冻结模型也在 GPU 上\n",
    "    frozen_dqn.model.to(DEVICE)\n",
    "    frozen_dqn.model.eval()\n",
    "    print(\"Loaded frozen DQN on GPU.\")\n",
    "else:\n",
    "    print(\"No DQN checkpoint provided.\")\n",
    "\n",
    "frozen_az = None\n",
    "if ckpt_az and ckpt_az.exists() and create_alphazero_model:\n",
    "    try:\n",
    "        frozen_az = create_alphazero_model('full')  # 如权重是轻量版改为 'light'\n",
    "        state = torch.load(ckpt_az, map_location=DEVICE)\n",
    "        if isinstance(state, dict) and 'model_state_dict' in state:\n",
    "            state = state['model_state_dict']\n",
    "        frozen_az.load_state_dict(state, strict=False)\n",
    "        frozen_az.to(DEVICE)\n",
    "        frozen_az.eval()\n",
    "        print(\"Loaded frozen AlphaZero on GPU.\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load AlphaZero model:\", e)\n",
    "        frozen_az = None\n",
    "else:\n",
    "    print(\"No AlphaZero checkpoint provided or loader unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义对手池\n",
    "包含多种难度层次的对手，重点训练搜索类对手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from kaggle_environments.envs.connectx.connectx import negamax_agent\n",
    "\n",
    "def random_policy(board, mark):\n",
    "    moves = get_valid_moves(board)\n",
    "    return random.choice(moves) if moves else 0\n",
    "\n",
    "def negamax_simple_policy(board, mark, depth=4):\n",
    "    # 原简化版：先手赢/堵 + 中心优先\n",
    "    return get_negamax_move(board, mark, depth=depth)\n",
    "\n",
    "def negamax_kaggle_policy(board, mark, depth=4):\n",
    "    # kaggle-environments 自带 negamax 搜索；深度通过配置传入\n",
    "    obs = SimpleNamespace(board=board, mark=mark)\n",
    "    cfg = SimpleNamespace(rows=ppo_config.ROWS, columns=ppo_config.COLUMNS, inarow=ppo_config.INAROW,\n",
    "                          timeout=1, actTimeout=1, depth=depth)\n",
    "    return int(negamax_agent(obs, cfg))\n",
    "\n",
    "def frozen_dqn_policy(board, mark):\n",
    "    assert frozen_dqn is not None, \"frozen_dqn not loaded\"\n",
    "    return frozen_dqn.select_action(board, mark, epsilon=0.0)\n",
    "\n",
    "def frozen_alphazero_policy(board, mark):\n",
    "    assert frozen_az is not None, \"frozen_az not loaded\"\n",
    "    # AlphaZero uses MCTS normally; here greedily pick best policy head\n",
    "    state = encode_state(board, mark)\n",
    "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = frozen_az(state_t)\n",
    "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    valid = get_valid_moves(board)\n",
    "    masked = np.full_like(probs, -1e9)\n",
    "    masked[valid] = probs[valid]\n",
    "    return int(masked.argmax())\n",
    "\n",
    "# GPU 加速训练：大幅提高搜索对手的采样权重\n",
    "opponent_candidates = [\n",
    "    (\"negamax_simple\", negamax_simple_policy, 0.15),\n",
    "    (\"negamax_kaggle\", negamax_kaggle_policy, 0.40),  # 重点训练搜索对手\n",
    "    (\"random\", random_policy, 0.20),\n",
    "]\n",
    "if frozen_dqn:\n",
    "    opponent_candidates.append((\"frozen_dqn\", frozen_dqn_policy, 0.125))\n",
    "if frozen_az:\n",
    "    opponent_candidates.append((\"frozen_az\", frozen_alphazero_policy, 0.125))\n",
    "\n",
    "# 归一化权重，确保总和为 1\n",
    "opponent_fns = [fn for _, fn, _ in opponent_candidates]\n",
    "opponent_weights = np.array([w for _, _, w in opponent_candidates], dtype=float)\n",
    "opponent_weights = opponent_weights / opponent_weights.sum()\n",
    "\n",
    "opponent_pool = [name for name, _, _ in opponent_candidates]\n",
    "print(\"Opponent pool:\", opponent_pool)\n",
    "print(\"Weights:\", opponent_weights)\n",
    "print(f\"Total opponents: {len(opponent_pool)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GPU 加速训练配置\n",
    "使用向量化环境进行批量 rollout，大幅提升 GPU 利用率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "agent = PPOAgent()\n",
    "\n",
    "# GPU 加速配置\n",
    "TOTAL_UPDATES = 500              # 总训练步数\n",
    "ROLLOUT_STEPS = 2048             # 单次 rollout 步数（增加以提高 GPU 利用率）\n",
    "USE_VECTORIZED = True           # 使用向量化环境\n",
    "NUM_VECTORIZED_ENVS = 64        # 向量化环境数量（根据 GPU 内存调整）\n",
    "\n",
    "# PPO 超参数优化\n",
    "ppo_config.LR = 1e-4            # 学习率\n",
    "ppo_config.PPO_EPOCHS = 4       # PPO 更新轮次\n",
    "ppo_config.MINI_BATCHES = 8     # mini-batch 数量\n",
    "ppo_config.CLIP_RANGE = 0.2     # PPO clipping 范围\n",
    "ppo_config.VF_COEF = 0.5        # value loss 权重\n",
    "ppo_config.ENT_COEF = 0.01      # entropy 权重\n",
    "ppo_config.MAX_GRAD_NORM = 0.5  # 梯度裁剪\n",
    "\n",
    "LOG_INTERVAL = 10\n",
    "reward_log = []\n",
    "performance_stats = {\n",
    "    'gpu_utilization': [],\n",
    "    'step_time': [],\n",
    "    'memory_usage': []\n",
    "}\n",
    "\n",
    "print(\"GPU Accelerated PPO Training Configuration:\")\n",
    "print(f\"Total Updates: {TOTAL_UPDATES}\")\n",
    "print(f\"Rollout Steps: {ROLLOUT_STEPS}\")\n",
    "print(f\"Vectorized Envs: {NUM_VECTORIZED_ENVS if USE_VECTORIZED else 'Disabled'}\")\n",
    "print(f\"Learning Rate: {ppo_config.LR}\")\n",
    "print(f\"Device: {ppo_config.DEVICE}\")\n",
    "\n",
    "# 预热 GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    warmup_tensor = torch.randn(1000, 1000, device=DEVICE)\n",
    "    del warmup_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU warmed up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GPU 加速训练循环\n",
    "监控 GPU 利用率、内存使用和训练性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for update in range(1, TOTAL_UPDATES + 1):\n",
    "    step_start = time.time()\n",
    "    \n",
    "    # 采样对手\n",
    "    opp_fn = random.choices(opponent_fns, weights=opponent_weights, k=1)[0]\n",
    "    \n",
    "    # GPU 加速 rollout\n",
    "    if USE_VECTORIZED:\n",
    "        batch = agent.generate_vectorized_rollout(opp_fn, ROLLOUT_STEPS, NUM_VECTORIZED_ENVS)\n",
    "    else:\n",
    "        batch = agent.generate_rollout(opp_fn, ROLLOUT_STEPS)\n",
    "    \n",
    "    # PPO 更新\n",
    "    metrics = agent.update(batch)\n",
    "    reward_log.append(batch.returns.mean().item())\n",
    "    \n",
    "    step_time = time.time() - step_start\n",
    "    performance_stats['step_time'].append(step_time)\n",
    "    \n",
    "    # 监控 GPU 状态\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_util = torch.cuda.utilization(DEVICE) if hasattr(torch.cuda, 'utilization') else 0\n",
    "        memory_used = torch.cuda.memory_allocated(DEVICE) / 1024**3  # GB\n",
    "        memory_total = torch.cuda.get_device_properties(DEVICE).total_memory / 1024**3\n",
    "        performance_stats['gpu_utilization'].append(gpu_util)\n",
    "        performance_stats['memory_usage'].append(memory_used)\n",
    "    \n",
    "    if update % LOG_INTERVAL == 0:\n",
    "        avg_ret = float(np.mean(reward_log[-LOG_INTERVAL:]))\n",
    "        avg_step_time = np.mean(performance_stats['step_time'][-LOG_INTERVAL:])\n",
    "        \n",
    "        # 安全地获取指标\n",
    "        if isinstance(metrics, dict):\n",
    "            loss_val = metrics.get(\"loss\", metrics.get(\"total_loss\", 0.0))\n",
    "            policy = metrics.get(\"policy_loss\", float(\"nan\"))\n",
    "            value = metrics.get(\"value_loss\", float(\"nan\"))\n",
    "            ent = metrics.get(\"entropy\", float(\"nan\"))\n",
    "            kl = metrics.get(\"approx_kl\", float(\"nan\"))\n",
    "            clip = metrics.get(\"clip_frac\", float(\"nan\"))\n",
    "        else:\n",
    "            loss_val = float(metrics)\n",
    "            policy = value = ent = kl = clip = float(\"nan\")\n",
    "        \n",
    "        # GPU 性能指标\n",
    "        gpu_info = \"\"\n",
    "        if torch.cuda.is_available() and performance_stats['gpu_utilization']:\n",
    "            avg_gpu = np.mean(performance_stats['gpu_utilization'][-LOG_INTERVAL:])\n",
    "            avg_mem = np.mean(performance_stats['memory_usage'][-LOG_INTERVAL:])\n",
    "            gpu_info = f\" | GPU {avg_gpu:.1f}% | Mem {avg_mem:.1f}GB\"\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        eta = (elapsed / update) * (TOTAL_UPDATES - update)\n",
    "        \n",
    "        print(\n",
    "            f\"Update {update}/{TOTAL_UPDATES} \"\n",
    "            f\"| loss {loss_val:.3f} \"\n",
    "            f\"| policy {policy:.3f} \"\n",
    "            f\"| value {value:.3f} \"\n",
    "            f\"| ent {ent:.3f} \"\n",
    "            f\"| kl {kl:.4f} \"\n",
    "            f\"| clip {clip:.3f} \"\n",
    "            f\"| avg_ret {avg_ret:.3f} \"\n",
    "            f\"| step {avg_step_time:.2f}s\"\n",
    "            f\"{gpu_info} \"\n",
    "            f\"| ETA {eta/3600:.1f}h\"\n",
    "        )\n",
    "\n",
    "# 保存模型\n",
    "ppo_path = Path('/kaggle/working/ppo_gpu_accelerated.pth')\n",
    "torch.save(agent.model.state_dict(), ppo_path)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {total_time/3600:.1f} hours\")\n",
    "print(f\"Average step time: {np.mean(performance_stats['step_time']):.2f}s\")\n",
    "if performance_stats['gpu_utilization']:\n",
    "    print(f\"Average GPU utilization: {np.mean(performance_stats['gpu_utilization']):.1f}%\")\n",
    "print(\"Saved\", ppo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GPU 加速评估\n",
    "使用向量化评估快速测试模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def play_one(policy_fn, opp_fn, games=50):\n",
    "    \"\"\"GPU 加速评估：增加对局数以获得更稳定的胜率估计\"\"\"\n",
    "    wins = 0\n",
    "    for g in range(games):\n",
    "        board = [0]*(ppo_config.ROWS*ppo_config.COLUMNS)\n",
    "        # 轮流先手：偶数局 PPO 先手(标记1)，奇数局对手先手(标记1)，PPO 用标记2\n",
    "        ppo_first = (g % 2 == 0)\n",
    "        ppo_mark = 1 if ppo_first else 2\n",
    "        current = 1\n",
    "        while True:\n",
    "            if current == ppo_mark:\n",
    "                action = policy_fn(board, ppo_mark)\n",
    "            else:\n",
    "                action = opp_fn(board, current)\n",
    "            board = make_move(board, action, current)\n",
    "            done, winner = is_terminal(board)\n",
    "            if done:\n",
    "                if winner == ppo_mark:\n",
    "                    wins += 1\n",
    "                break\n",
    "            current = 3 - current\n",
    "    return wins / games\n",
    "\n",
    "def ppo_policy(board, mark):\n",
    "    state = encode_state(board, mark)\n",
    "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = agent.model(state_t)\n",
    "        valid = get_valid_moves(board)\n",
    "        mask = torch.full_like(logits, float('-inf'))\n",
    "        mask[0, valid] = 0\n",
    "        logits = logits + mask\n",
    "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    return int(np.random.choice(len(probs), p=probs))\n",
    "\n",
    "# GPU 加速评估\n",
    "print(\"GPU Accelerated Model Evaluation:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 基础对手\n",
    "print(\"PPO vs random:\", play_one(ppo_policy, random_policy, games=50))\n",
    "\n",
    "# 搜索对手\n",
    "print(\"PPO vs negamax_simple:\", play_one(ppo_policy, lambda b,m: negamax_simple_policy(b,m,depth=4), games=50))\n",
    "print(\"PPO vs negamax_kaggle:\", play_one(ppo_policy, lambda b,m: negamax_kaggle_policy(b,m,depth=4), games=50))\n",
    "\n",
    "# 学习对手\n",
    "if frozen_dqn:\n",
    "    print(\"PPO vs frozen DQN:\", play_one(ppo_policy, frozen_dqn_policy, games=50))\n",
    "if frozen_az:\n",
    "    print(\"PPO vs frozen AlphaZero:\", play_one(ppo_policy, frozen_alphazero_policy, games=50))\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Evaluation completed with GPU acceleration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能分析\n",
    "分析训练过程中的 GPU 利用率和性能指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练曲线\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 奖励曲线\n",
    "axes[0,0].plot(reward_log, alpha=0.7)\n",
    "axes[0,0].set_title('Training Reward')\n",
    "axes[0,0].set_xlabel('Update')\n",
    "axes[0,0].set_ylabel('Average Return')\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# GPU 利用率\n",
    "if performance_stats['gpu_utilization']:\n",
    "    gpu_log = performance_stats['gpu_utilization']\n",
    "    axes[0,1].plot(gpu_log, alpha=0.7, color='green')\n",
    "    axes[0,1].set_title('GPU Utilization')\n",
    "    axes[0,1].set_xlabel('Update')\n",
    "    axes[0,1].set_ylabel('GPU %')\n",
    "    axes[0,1].set_ylim(0, 100)\n",
    "    axes[0,1].grid(True)\n",
    "    axes[0,1].axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50% threshold')\n",
    "    axes[0,1].legend()\n",
    "\n",
    "# 内存使用\n",
    "if performance_stats['memory_usage']:\n",
    "    mem_log = performance_stats['memory_usage']\n",
    "    axes[1,0].plot(mem_log, alpha=0.7, color='orange')\n",
    "    axes[1,0].set_title('GPU Memory Usage')\n",
    "    axes[1,0].set_xlabel('Update')\n",
    "    axes[1,0].set_ylabel('Memory (GB)')\n",
    "    axes[1,0].grid(True)\n",
    "\n",
    "# 步长时间\n",
    "step_times = performance_stats['step_time']\n",
    "axes[1,1].plot(step_times, alpha=0.7, color='purple')\n",
    "axes[1,1].set_title('Step Time')\n",
    "axes[1,1].set_xlabel('Update')\n",
    "axes[1,1].set_ylabel('Time (s)')\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 性能统计\n",
    "print(\"\\nGPU Accelerated Training Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total training time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Average step time: {np.mean(step_times):.3f}s\")\n",
    "print(f\"Total steps: {len(step_times)}\")\n",
    "print(f\"Total environment interactions: {len(step_times) * ROLLOUT_STEPS * (NUM_VECTORIZED_ENVS if USE_VECTORIZED else 1):,}\")\n",
    "\n",
    "if performance_stats['gpu_utilization']:\n",
    "    print(f\"Average GPU utilization: {np.mean(gpu_log):.1f}%\")\n",
    "    print(f\"Peak GPU utilization: {np.max(gpu_log):.1f}%\")\n",
    "\n",
    "if performance_stats['memory_usage']:\n",
    "    print(f\"Average GPU memory: {np.mean(mem_log):.2f} GB\")\n",
    "    print(f\"Peak GPU memory: {np.max(mem_log):.2f} GB\")\n",
    "\n",
    "print(f\"Final average reward: {np.mean(reward_log[-50:]):.3f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 上传到 Hugging Face\n",
    "保存训练好的 GPU 加速模型到 HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# 设置你的 HF token（安全方式：使用环境变量）\n",
    "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\"\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\", \"\")\n",
    "REPO_ID = \"your-username/connectx-ppo-gpu-accelerated\"  # 替换为你的仓库\n",
    "\n",
    "if HF_TOKEN and REPO_ID:\n",
    "    api = HfApi()\n",
    "    HfFolder.save_token(HF_TOKEN)\n",
    "    \n",
    "    # 创建或获取仓库\n",
    "    try:\n",
    "        api.create_repo(repo_id=REPO_ID, repo_type=\"model\", private=False)\n",
    "        print(f\"Created HF repo: {REPO_ID}\")\n",
    "    except:\n",
    "        print(f\"HF repo {REPO_ID} already exists\")\n",
    "    \n",
    "    # 上传模型\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=str(ppo_path),\n",
    "        path_in_repo=\"ppo_gpu_accelerated.pth\",\n",
    "        repo_id=REPO_ID,\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    print(f\"Uploaded GPU-accelerated model to HF: {REPO_ID}\")\n",
    "    \n",
    "    # 创建模型卡片\n",
    "    model_card = f\"\"\"\n",
    "---\n",
    "tags:\n",
    "- connectx\n",
    "- reinforcement-learning\n",
    "- ppo\n",
    "- gpu-accelerated\n",
    "library_name: pytorch\n",
    "---\n",
    "\n",
    "# ConnectX PPO Agent (GPU Accelerated)\n",
    "\n",
    "This model was trained using GPU-accelerated PPO with vectorized environments for maximum training efficiency.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Algorithm**: Proximal Policy Optimization (PPO)\n",
    "- **Environment**: ConnectX (6x7 grid, 4-in-a-row)\n",
    "- **Training**: GPU-accelerated with {NUM_VECTORIZED_ENVS} parallel environments\n",
    "- **Architecture**: Large CNN (256→512→512 channels) + FC layers\n",
    "- **Parameters**: {sum(p.numel() for p in ActorCriticGPU().parameters()):,}\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "- Total Updates: {TOTAL_UPDATES}\n",
    "- Rollout Steps: {ROLLOUT_STEPS}\n",
    "- Learning Rate: {ppo_config.LR}\n",
    "- Vectorized Environments: {NUM_VECTORIZED_ENVS}\n",
    "- GPU Memory: ~{np.max(performance_stats['memory_usage']):.1f} GB peak\n",
    "- Training Time: {total_time/3600:.1f} hours\n",
    "\n",
    "## Performance\n",
    "\n",
    "- Average GPU Utilization: {np.mean(performance_stats['gpu_utilization']):.1f}%\n",
    "- Final Average Reward: {np.mean(reward_log[-50:]):.3f}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from agents.ppo.ppo_model import make_model\n",
    "\n",
    "model = make_model()\n",
    "model.load_state_dict(torch.load('ppo_gpu_accelerated.pth'))\n",
    "model.eval()\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    # 上传模型卡片\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=model_card,\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=REPO_ID,\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    print(\"Uploaded model card\")\n",
    "    \n",
    "else:\n",
    "    print(\"HF token or repo id not set; skip upload.\")\n",
    "    print(\"To upload, set HUGGINGFACE_HUB_TOKEN environment variable and update REPO_ID\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
