{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO Training with Adaptive Opponent Sampling (win-rate prioritized)\n",
        "\n",
        "基于 `ppo_frozen_opponents.ipynb` 的变体：对手池按实时胜率动态加权，优先挑战当前胜率较低的对手，其余流程保持一致。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境与依赖\n",
        "- 需 Kaggle GPU 环境\n",
        "- 依赖：`kaggle-environments`, `huggingface_hub`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/mogoo7zn/Kaggle-ConnectX.git\n",
        "%cd Kaggle-ConnectX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q kaggle-environments huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "\n",
        "# 让项目包可被导入：优先常见 Kaggle 路径，否则尝试当前/父目录\n",
        "candidates = [\n",
        "    Path('/kaggle/working/Kaggle-ConnectX'),\n",
        "    Path('/kaggle/working'),\n",
        "    Path.cwd(),\n",
        "    Path.cwd().parent,\n",
        "]\n",
        "repo_root = None\n",
        "for c in candidates:\n",
        "    if (c / 'agents').exists():\n",
        "        repo_root = c\n",
        "        break\n",
        "if repo_root is None:\n",
        "    repo_root = Path('.').resolve()\n",
        "\n",
        "os.chdir(repo_root)\n",
        "sys.path.insert(0, str(repo_root))\n",
        "importlib.invalidate_caches()\n",
        "print(\"Repo root:\", repo_root)\n",
        "print(\"CWD:\", Path.cwd())\n",
        "\n",
        "from agents.ppo.ppo_agent import PPOAgent\n",
        "from agents.ppo.ppo_config import ppo_config\n",
        "from agents.base.utils import get_valid_moves, get_negamax_move, encode_state, make_move, is_terminal\n",
        "from agents.dqn.dqn_agent import DQNAgent\n",
        "\n",
        "# 可选：AlphaZero 载入（需要与你队友的权重匹配）\n",
        "try:\n",
        "    from agents.alphazero.az_model import create_alphazero_model\n",
        "except Exception:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    create_alphazero_model = None\n",
        "\n",
        "DEVICE = ppo_config.DEVICE\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可在 notebook 内直接定义/替换模型\n",
        "import torch.nn as nn\n",
        "from agents.ppo import ppo_model\n",
        "\n",
        "class ActorCriticCustom(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 128, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        conv_out = ppo_config.ROWS * ppo_config.COLUMNS * 256\n",
        "        self.fc = nn.Linear(conv_out, 512)\n",
        "        self.policy = nn.Linear(512, ppo_config.COLUMNS)\n",
        "        self.value = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc(x))\n",
        "        logits = self.policy(x)\n",
        "        value = self.value(x)\n",
        "        return logits, value\n",
        "\n",
        "# 覆盖原有工厂方法（如不需要可注释掉）\n",
        "ppo_model.ActorCritic = ActorCriticCustom\n",
        "ppo_model.make_model = lambda: ActorCriticCustom().to(ppo_config.DEVICE)\n",
        "\n",
        "print(\"Using custom ActorCritic with larger capacity:\", ActorCriticCustom())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "# 直接填入公开仓库的 resolve 链接\n",
        "DQN_URL = \"https://huggingface.co/mogoo7zn/Kaggle-ConnectX/resolve/main/DQN-base.pth\"\n",
        "AZ_URL  = \"https://huggingface.co/mogoo7zn/Kaggle-ConnectX/resolve/main/alpha-zero-medium.pth\"\n",
        "\n",
        "ckpt_dir = Path('/kaggle/working/checkpoints')\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def download(url, out_path):\n",
        "    if not url:\n",
        "        print(f\"[skip] empty url for {out_path.name}\")\n",
        "        return None\n",
        "    print(f\"Downloading {url} -> {out_path}\")\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    with open(out_path, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return out_path\n",
        "\n",
        "ckpt_dqn = download(DQN_URL, ckpt_dir/'dqn_frozen.pth')\n",
        "ckpt_az  = download(AZ_URL,  ckpt_dir/'alphazero_frozen.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frozen_dqn = None\n",
        "if ckpt_dqn and ckpt_dqn.exists():\n",
        "    frozen_dqn = DQNAgent()\n",
        "    frozen_dqn.load_model(str(ckpt_dqn))\n",
        "    print(\"Loaded frozen DQN.\")\n",
        "else:\n",
        "    print(\"No DQN checkpoint provided.\")\n",
        "\n",
        "frozen_az = None\n",
        "if ckpt_az and ckpt_az.exists() and create_alphazero_model:\n",
        "    try:\n",
        "        frozen_az = create_alphazero_model('full')  # 如权重是轻量版改为 'light'\n",
        "        state = torch.load(ckpt_az, map_location=DEVICE)\n",
        "        if isinstance(state, dict) and 'model_state_dict' in state:\n",
        "            state = state['model_state_dict']\n",
        "        frozen_az.load_state_dict(state, strict=False)\n",
        "        frozen_az.eval()\n",
        "        print(\"Loaded frozen AlphaZero model.\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to load AlphaZero model:\", e)\n",
        "        frozen_az = None\n",
        "else:\n",
        "    print(\"No AlphaZero checkpoint provided or loader unavailable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# 额外冻结 PPO 对手（来自个人仓库）\n",
        "hf_ppo_sources = [\n",
        "    (\"ppo_mix\", \"tigerxin024/connectx-ppo-frozen\", \"ppo_frozen_mix.pth\"),\n",
        "    (\"ppo_mix_v16\", \"tigerxin024/connectx-ppo-frozen\", \"ppo_frozen_mix_v1.6.pth\"),\n",
        "]\n",
        "\n",
        "frozen_ppo_models = {}\n",
        "\n",
        "for name, repo_id, filename in hf_ppo_sources:\n",
        "    try:\n",
        "        ckpt = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "        model = PPOAgent()\n",
        "        state = torch.load(ckpt, map_location=DEVICE)\n",
        "        if isinstance(state, dict) and \"state_dict\" in state:\n",
        "            state = state[\"state_dict\"]\n",
        "        model.model.load_state_dict(state, strict=False)\n",
        "        model.model.eval()\n",
        "        frozen_ppo_models[name] = model\n",
        "        print(f\"Loaded frozen PPO from HF: {name} ({repo_id}/{filename})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load HF PPO {name}: {e}\")\n",
        "\n",
        "\n",
        "def make_frozen_ppo_policy(model: PPOAgent):\n",
        "    def policy(board, mark):\n",
        "        state = encode_state(board, mark)\n",
        "        state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model.model(state_t)\n",
        "            valid = get_valid_moves(board)\n",
        "            mask = torch.full_like(logits, float('-inf'))\n",
        "            mask[0, valid] = 0\n",
        "            logits = logits + mask\n",
        "            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "        return int(np.random.choice(len(probs), p=probs))\n",
        "    return policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "from kaggle_environments.envs.connectx.connectx import negamax_agent\n",
        "\n",
        "# 定义固定策略对手\n",
        "def random_policy(board, mark):\n",
        "    moves = get_valid_moves(board)\n",
        "    return random.choice(moves) if moves else 0\n",
        "\n",
        "def negamax_simple_policy(board, mark, depth=4):\n",
        "    return get_negamax_move(board, mark, depth=depth)\n",
        "\n",
        "def negamax_kaggle_policy(board, mark, depth=4):\n",
        "    obs = SimpleNamespace(board=board, mark=mark)\n",
        "    cfg = SimpleNamespace(rows=ppo_config.ROWS, columns=ppo_config.COLUMNS, inarow=ppo_config.INAROW,\n",
        "                          timeout=1, actTimeout=1, depth=depth)\n",
        "    return int(negamax_agent(obs, cfg))\n",
        "\n",
        "def frozen_dqn_policy(board, mark):\n",
        "    assert frozen_dqn is not None\n",
        "    return frozen_dqn.select_action(board, mark, epsilon=0.0)\n",
        "\n",
        "def frozen_alphazero_policy(board, mark):\n",
        "    assert frozen_az is not None\n",
        "    state = encode_state(board, mark)\n",
        "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = frozen_az(state_t)\n",
        "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    valid = get_valid_moves(board)\n",
        "    masked = np.full_like(probs, -1e9)\n",
        "    masked[valid] = probs[valid]\n",
        "    return int(masked.argmax())\n",
        "\n",
        "# 对手表 + 统计，初始胜率视为 0.5（wins=1,games=2 防止除0）\n",
        "opponent_registry = [\n",
        "    (\"negamax_simple\", negamax_simple_policy),\n",
        "    (\"negamax_kaggle\", negamax_kaggle_policy),\n",
        "    (\"random\", random_policy),\n",
        "]\n",
        "if frozen_dqn:\n",
        "    opponent_registry.append((\"frozen_dqn\", frozen_dqn_policy))\n",
        "if frozen_az:\n",
        "    opponent_registry.append((\"frozen_az\", frozen_alphazero_policy))\n",
        "if frozen_ppo_models:\n",
        "    for name, model in frozen_ppo_models.items():\n",
        "        opponent_registry.append((f\"hf_{name}\", make_frozen_ppo_policy(model)))\n",
        "\n",
        "opponent_stats = {name: {\"wins\": 1, \"games\": 2} for name, _ in opponent_registry}\n",
        "\n",
        "\n",
        "def sample_opponent(stats):\n",
        "    names = list(stats.keys())\n",
        "    win_rates = np.array([v[\"wins\"] / v[\"games\"] for v in stats.values()], dtype=float)\n",
        "    # 越难打（胜率低）权重越高，加上平滑项避免全 0\n",
        "    difficulties = 1.0 - win_rates\n",
        "    probs = difficulties + 1e-3\n",
        "    probs = probs / probs.sum()\n",
        "    idx = np.random.choice(len(names), p=probs)\n",
        "    return names[idx], probs\n",
        "\n",
        "\n",
        "def record_result(name, win_rate_batch):\n",
        "    stats = opponent_stats[name]\n",
        "    games_add = 1\n",
        "    stats[\"games\"] += games_add\n",
        "    stats[\"wins\"] += win_rate_batch * games_add\n",
        "\n",
        "\n",
        "print(\"Opponents:\", list(opponent_stats.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = PPOAgent()\n",
        "\n",
        "TOTAL_UPDATES = 100\n",
        "ROLLOUT_STEPS = 1024\n",
        "LOG_INTERVAL = 10\n",
        "EVAL_GAMES = 6\n",
        "\n",
        "reward_log = []\n",
        "\n",
        "# 简单评估函数：PPO 始终用 mark=1 先手\n",
        "\n",
        "def eval_vs(opp_fn, games=EVAL_GAMES):\n",
        "    wins = 0\n",
        "    for g in range(games):\n",
        "        board = [0] * (ppo_config.ROWS * ppo_config.COLUMNS)\n",
        "        current = 1\n",
        "        while True:\n",
        "            if current == 1:\n",
        "                action, _, _ = agent.select_action(board, current)\n",
        "            else:\n",
        "                action = opp_fn(board, current)\n",
        "            board = make_move(board, action, current)\n",
        "            done, winner = is_terminal(board)\n",
        "            if done:\n",
        "                if winner == 1:\n",
        "                    wins += 1\n",
        "                break\n",
        "            current = 3 - current\n",
        "    return wins / games\n",
        "\n",
        "\n",
        "for update in range(1, TOTAL_UPDATES + 1):\n",
        "    opp_name, probs = sample_opponent(opponent_stats)\n",
        "    opp_fn = dict(opponent_registry)[opp_name]\n",
        "\n",
        "    batch = agent.generate_rollout(opp_fn, ROLLOUT_STEPS)\n",
        "    metrics = agent.update(batch)\n",
        "    reward_log.append(batch.returns.mean().item())\n",
        "\n",
        "    # 训练后立刻小样本评估该对手，更新胜率\n",
        "    win_rate_batch = eval_vs(opp_fn, games=EVAL_GAMES)\n",
        "    record_result(opp_name, win_rate_batch)\n",
        "\n",
        "    if update % LOG_INTERVAL == 0:\n",
        "        avg_ret = float(np.mean(reward_log[-LOG_INTERVAL:]))\n",
        "        if isinstance(metrics, dict):\n",
        "            loss_val = metrics.get(\"loss\", metrics.get(\"total_loss\", 0.0))\n",
        "            policy = metrics.get(\"policy_loss\", float(\"nan\"))\n",
        "            value = metrics.get(\"value_loss\", float(\"nan\"))\n",
        "            ent = metrics.get(\"entropy\", float(\"nan\"))\n",
        "            kl = metrics.get(\"approx_kl\", float(\"nan\"))\n",
        "            clip = metrics.get(\"clip_frac\", float(\"nan\"))\n",
        "        else:\n",
        "            loss_val = float(metrics)\n",
        "            policy = value = ent = kl = clip = float(\"nan\")\n",
        "\n",
        "        win_rates = {n: round(v[\"wins\"] / v[\"games\"], 3) for n, v in opponent_stats.items()}\n",
        "        print(\n",
        "            f\"Update {update}/{TOTAL_UPDATES} \"\n",
        "            f\"| opp {opp_name} \"\n",
        "            f\"| loss {loss_val:.3f} \"\n",
        "            f\"| avg_ret {avg_ret:.3f} \"\n",
        "            f\"| win_rates {win_rates} \"\n",
        "            f\"| probs {[round(p,3) for p in probs.tolist()]}\"\n",
        "        )\n",
        "\n",
        "# 保存模型\n",
        "ppo_path = Path('/kaggle/working/ppo_frozen_mix_adaptive.pth')\n",
        "torch.save(agent.model.state_dict(), ppo_path)\n",
        "print(\"Saved\", ppo_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 评估：训练始终先手（上方已满足）。当模型处于后手时，改用固定求和策略尽量逼平。\n",
        "# 这里选用 negamax_kaggle 作为后手保平策略，可按需更换。\n",
        "draw_policy = lambda board, mark: negamax_kaggle_policy(board, mark, depth=4)\n",
        "\n",
        "\n",
        "def play_one(policy_first_fn, opp_fn, games=20, alt_first=True, draw_on_second=None):\n",
        "    wins = 0\n",
        "    draws = 0\n",
        "    for g in range(games):\n",
        "        board = [0]*(ppo_config.ROWS*ppo_config.COLUMNS)\n",
        "        # 轮流先手：偶数局 PPO 先手(标记1)，奇数局对手先手(标记1)，PPO 用标记2\n",
        "        ppo_first = (g % 2 == 0) if alt_first else True\n",
        "        ppo_mark = 1 if ppo_first else 2\n",
        "        current = 1\n",
        "        while True:\n",
        "            if current == ppo_mark:\n",
        "                action = policy_first_fn(board, ppo_mark)\n",
        "            else:\n",
        "                if draw_on_second is not None and ppo_mark == 2:\n",
        "                    action = draw_on_second(board, current)\n",
        "                else:\n",
        "                    action = opp_fn(board, current)\n",
        "            board = make_move(board, action, current)\n",
        "            done, winner = is_terminal(board)\n",
        "            if done:\n",
        "                if winner == ppo_mark:\n",
        "                    wins += 1\n",
        "                elif winner is None or winner == 0:\n",
        "                    draws += 1\n",
        "                break\n",
        "            current = 3 - current\n",
        "    return wins / games, draws / games\n",
        "\n",
        "\n",
        "def ppo_policy(board, mark):\n",
        "    state = encode_state(board, mark)\n",
        "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = agent.model(state_t)\n",
        "        valid = get_valid_moves(board)\n",
        "        mask = torch.full_like(logits, float('-inf'))\n",
        "        mask[0, valid] = 0\n",
        "        logits = logits + mask\n",
        "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    return int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "# 纯模型（交替先后）\n",
        "wr, dr = play_one(ppo_policy, random_policy, games=20)\n",
        "print(f\"PPO vs random | win {wr:.3f} | draw {dr:.3f}\")\n",
        "wr, dr = play_one(ppo_policy, lambda b,m: negamax_simple_policy(b,m,depth=4), games=20)\n",
        "print(f\"PPO vs negamax_simple | win {wr:.3f} | draw {dr:.3f}\")\n",
        "wr, dr = play_one(ppo_policy, lambda b,m: negamax_kaggle_policy(b,m,depth=4), games=20)\n",
        "print(f\"PPO vs negamax_kaggle | win {wr:.3f} | draw {dr:.3f}\")\n",
        "if frozen_dqn:\n",
        "    wr, dr = play_one(ppo_policy, frozen_dqn_policy, games=20)\n",
        "    print(f\"PPO vs frozen DQN | win {wr:.3f} | draw {dr:.3f}\")\n",
        "if frozen_az:\n",
        "    wr, dr = play_one(ppo_policy, frozen_alphazero_policy, games=20)\n",
        "    print(f\"PPO vs frozen AlphaZero | win {wr:.3f} | draw {dr:.3f}\")\n",
        "\n",
        "# 模型先手、后手用 draw_policy（目的：后手保平）\n",
        "wr, dr = play_one(ppo_policy, random_policy, games=20, draw_on_second=draw_policy)\n",
        "print(f\"(1st model / 2nd draw_policy) vs random | win {wr:.3f} | draw {dr:.3f}\")\n",
        "wr, dr = play_one(ppo_policy, lambda b,m: negamax_simple_policy(b,m,depth=4), games=20, draw_on_second=draw_policy)\n",
        "print(f\"(1st model / 2nd draw_policy) vs negamax_simple | win {wr:.3f} | draw {dr:.3f}\")\n",
        "wr, dr = play_one(ppo_policy, lambda b,m: negamax_kaggle_policy(b,m,depth=4), games=20, draw_on_second=draw_policy)\n",
        "print(f\"(1st model / 2nd draw_policy) vs negamax_kaggle | win {wr:.3f} | draw {dr:.3f}\")\n",
        "if frozen_dqn:\n",
        "    wr, dr = play_one(ppo_policy, frozen_dqn_policy, games=20, draw_on_second=draw_policy)\n",
        "    print(f\"(1st model / 2nd draw_policy) vs frozen DQN | win {wr:.3f} | draw {dr:.3f}\")\n",
        "if frozen_az:\n",
        "    wr, dr = play_one(ppo_policy, frozen_alphazero_policy, games=20, draw_on_second=draw_policy)\n",
        "    print(f\"(1st model / 2nd draw_policy) vs frozen AlphaZero | win {wr:.3f} | draw {dr:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "# 需先在上方运行 `notebook_login()` 完成认证\n",
        "to_upload = ppo_path\n",
        "hub_model_id = \"your-username/connectx-ppo-adaptive\"  # 替换为你的仓库 ID\n",
        "create_repo(hub_model_id, exist_ok=True)\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=str(to_upload),\n",
        "    path_in_repo=to_upload.name,\n",
        "    repo_id=hub_model_id,\n",
        ")\n",
        "print(\"Uploaded .pth to HF repo:\", hub_model_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "hf_repo_id = hub_model_id  # 或者填你上传的仓库\n",
        "hf_ckpt = hf_hub_download(repo_id=hf_repo_id, filename=to_upload.name)\n",
        "print(\"Downloaded from HF:\", hf_ckpt)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
