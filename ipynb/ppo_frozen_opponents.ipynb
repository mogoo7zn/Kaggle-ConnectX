{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO Training with Frozen DQN / AlphaZero Opponents\n",
        "\n",
        "在 Kaggle GPU 上运行：下载队友训练好的 DQN / AlphaZero 权重，作为冻结对手加入对手池，与 PPO 训练对战，最后保存/上传权重到 Hugging Face。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 克隆代码仓库（云端首次运行需要）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mogoo7zn/Kaggle-ConnectX.git\n",
        "%cd Kaggle-ConnectX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境与依赖\n",
        "- 需 Kaggle GPU 环境\n",
        "- 安装最小依赖：`kaggle-environments`（对战模拟）、`huggingface_hub`（可选上传）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle-environments huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "\n",
        "# 让项目包可被导入：优先常见 Kaggle 路径，否则尝试当前/父目录\n",
        "candidates = [\n",
        "    Path('/kaggle/working/Kaggle-ConnectX'),  # git clone 默认位置\n",
        "    Path('/kaggle/working'),                  # 若 notebook 已位于仓库内，这里无 agents 则会跳过\n",
        "    Path.cwd(),\n",
        "    Path.cwd().parent,\n",
        "]\n",
        "repo_root = None\n",
        "for c in candidates:\n",
        "    if (c / 'agents').exists():\n",
        "        repo_root = c\n",
        "        break\n",
        "if repo_root is None:\n",
        "    repo_root = Path('.').resolve()\n",
        "\n",
        "os.chdir(repo_root)\n",
        "sys.path.insert(0, str(repo_root))\n",
        "importlib.invalidate_caches()\n",
        "print(\"Repo root:\", repo_root)\n",
        "print(\"CWD:\", Path.cwd())\n",
        "print(\"agents exists:\", (repo_root / 'agents').exists())\n",
        "print(\"mcts exists:\", (repo_root / 'agents/alphazero/mcts.py').exists())\n",
        "\n",
        "from agents.ppo.ppo_agent import PPOAgent\n",
        "from agents.ppo.ppo_config import ppo_config\n",
        "from agents.base.utils import get_valid_moves, get_negamax_move, encode_state, make_move, is_terminal\n",
        "from agents.dqn.dqn_agent import DQNAgent\n",
        "\n",
        "# 可选：AlphaZero 载入（需要与你队友的权重匹配）\n",
        "try:\n",
        "    from agents.alphazero.az_model import create_alphazero_model\n",
        "except Exception:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    create_alphazero_model = None\n",
        "\n",
        "DEVICE = ppo_config.DEVICE\n",
        "print(\"Using device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 可在 notebook 内直接定义/替换模型，避免每次修改后重新 clone\n",
        "# 调整下方通道/隐藏层即可放大模型规模\n",
        "import types\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from agents.ppo import ppo_model\n",
        "\n",
        "class ActorCriticCustom(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 自行调大通道/隐藏层尺寸\n",
        "        self.conv1 = nn.Conv2d(3, 128, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        conv_out = ppo_config.ROWS * ppo_config.COLUMNS * 256\n",
        "        self.fc = nn.Linear(conv_out, 512)\n",
        "        self.policy = nn.Linear(512, ppo_config.COLUMNS)\n",
        "        self.value = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc(x))\n",
        "        logits = self.policy(x)\n",
        "        value = self.value(x)\n",
        "        return logits, value\n",
        "\n",
        "# 覆盖原有工厂方法\n",
        "ppo_model.ActorCritic = ActorCriticCustom\n",
        "ppo_model.make_model = lambda: ActorCriticCustom().to(ppo_config.DEVICE)\n",
        "\n",
        "print(\"Using custom ActorCritic with larger capacity:\", ActorCriticCustom())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 下载队友的已训练模型\n",
        "填写你队友提供的下载链接（例如 Hugging Face / Kaggle Dataset）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "# 直接填入公开仓库的 resolve 链接\n",
        "DQN_URL = \"https://huggingface.co/mogoo7zn/Kaggle-ConnectX/resolve/main/DQN-base.pth\"\n",
        "AZ_URL  = \"https://huggingface.co/mogoo7zn/Kaggle-ConnectX/resolve/main/alpha-zero-medium.pth\"\n",
        "# 如果你想用 high 版，就把上面这一行改成 alpha-zero-high.pth\n",
        "\n",
        "ckpt_dir = Path('/kaggle/working/checkpoints')\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def download(url, out_path):\n",
        "    if not url:\n",
        "        print(f\"[skip] empty url for {out_path.name}\")\n",
        "        return None\n",
        "    print(f\"Downloading {url} -> {out_path}\")\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    with open(out_path, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return out_path\n",
        "\n",
        "ckpt_dqn = download(DQN_URL, ckpt_dir/'dqn_frozen.pth')\n",
        "ckpt_az  = download(AZ_URL,  ckpt_dir/'alphazero_frozen.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 加载冻结对手（DQN / AlphaZero，可选）\n",
        "如果未提供权重，则跳过对应对手。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "frozen_dqn = None\n",
        "if ckpt_dqn and ckpt_dqn.exists():\n",
        "    frozen_dqn = DQNAgent()\n",
        "    frozen_dqn.load_model(str(ckpt_dqn))\n",
        "    print(\"Loaded frozen DQN.\")\n",
        "else:\n",
        "    print(\"No DQN checkpoint provided.\")\n",
        "\n",
        "frozen_az = None\n",
        "if ckpt_az and ckpt_az.exists() and create_alphazero_model:\n",
        "    try:\n",
        "        frozen_az = create_alphazero_model('full')  # 如权重是轻量版改为 'light'\n",
        "        state = torch.load(ckpt_az, map_location=DEVICE)\n",
        "        if isinstance(state, dict) and 'model_state_dict' in state:\n",
        "            state = state['model_state_dict']\n",
        "        frozen_az.load_state_dict(state, strict=False)\n",
        "        frozen_az.eval()\n",
        "        print(\"Loaded frozen AlphaZero model.\")\n",
        "    except Exception as e:\n",
        "        print(\"Failed to load AlphaZero model:\", e)\n",
        "        frozen_az = None\n",
        "else:\n",
        "    print(\"No AlphaZero checkpoint provided or loader unavailable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 定义对手池（随机 / negamax / 冻结DQN / 冻结AlphaZero）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "from kaggle_environments.envs.connectx.connectx import negamax_agent\n",
        "\n",
        "def random_policy(board, mark):\n",
        "    moves = get_valid_moves(board)\n",
        "    return random.choice(moves) if moves else 0\n",
        "\n",
        "def negamax_simple_policy(board, mark, depth=4):\n",
        "    # 原简化版：先手赢/堵 + 中心优先\n",
        "    return get_negamax_move(board, mark, depth=depth)\n",
        "\n",
        "def negamax_kaggle_policy(board, mark, depth=4):\n",
        "    # kaggle-environments 自带 negamax 搜索；深度通过配置传入（若实现不支持，则忽略）\n",
        "    obs = SimpleNamespace(board=board, mark=mark)\n",
        "    cfg = SimpleNamespace(rows=ppo_config.ROWS, columns=ppo_config.COLUMNS, inarow=ppo_config.INAROW,\n",
        "                          timeout=1, actTimeout=1, depth=depth)\n",
        "    return int(negamax_agent(obs, cfg))\n",
        "\n",
        "def frozen_dqn_policy(board, mark):\n",
        "    assert frozen_dqn is not None, \"frozen_dqn not loaded; otherwise fallback would be random\"\n",
        "    return frozen_dqn.select_action(board, mark, epsilon=0.0)\n",
        "\n",
        "def frozen_alphazero_policy(board, mark):\n",
        "    assert frozen_az is not None, \"frozen_az not loaded; otherwise fallback would be random\"\n",
        "    # AlphaZero uses MCTS normally; here greedily pick best policy head\n",
        "    state = encode_state(board, mark)\n",
        "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = frozen_az(state_t)\n",
        "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    valid = get_valid_moves(board)\n",
        "    masked = np.full_like(probs, -1e9)\n",
        "    masked[valid] = probs[valid]\n",
        "    return int(masked.argmax())\n",
        "\n",
        "# 目标：negamax 类对手合计 50% 抽样概率\n",
        "opponent_candidates = [\n",
        "    (\"negamax_simple\", negamax_simple_policy, 0.25),\n",
        "    (\"negamax_kaggle\", negamax_kaggle_policy, 0.25),\n",
        "    (\"random\", random_policy, 0.25),\n",
        "]\n",
        "if frozen_dqn:\n",
        "    opponent_candidates.append((\"frozen_dqn\", frozen_dqn_policy, 0.125))\n",
        "if frozen_az:\n",
        "    opponent_candidates.append((\"frozen_az\", frozen_alphazero_policy, 0.125))\n",
        "\n",
        "# 归一化权重，确保总和为 1\n",
        "opponent_fns = [fn for _, fn, _ in opponent_candidates]\n",
        "opponent_weights = np.array([w for _, _, w in opponent_candidates], dtype=float)\n",
        "opponent_weights = opponent_weights / opponent_weights.sum()\n",
        "\n",
        "opponent_pool = [name for name, _, _ in opponent_candidates]\n",
        "print(\"Opponent pool:\", opponent_pool)\n",
        "print(\"Weights:\", opponent_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 训练循环（小规模示例）\n",
        "- 从对手池随机采样对手\n",
        "- 收集 rollout（玩家1视角）\n",
        "- PPO 更新\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "agent = PPOAgent()\n",
        "\n",
        "TOTAL_UPDATES = 100   # 可调大以获得更强效果\n",
        "ROLLOUT_STEPS = 512   # 可调大\n",
        "LOG_INTERVAL = 10\n",
        "\n",
        "reward_log = []\n",
        "\n",
        "for update in range(1, TOTAL_UPDATES + 1):\n",
        "    opp_fn = random.choices(opponent_fns, weights=opponent_weights, k=1)[0]\n",
        "    batch = agent.generate_rollout(opp_fn, ROLLOUT_STEPS)\n",
        "    metrics = agent.update(batch)\n",
        "    reward_log.append(batch.returns.mean().item())\n",
        "\n",
        "    if update % LOG_INTERVAL == 0:\n",
        "        avg_ret = float(np.mean(reward_log[-LOG_INTERVAL:]))\n",
        "        # agent.update may return a float (old API) or a metrics dict; normalize for logging\n",
        "        if isinstance(metrics, dict):\n",
        "            loss_val = metrics.get(\"loss\", metrics.get(\"total_loss\", 0.0))\n",
        "            policy = metrics.get(\"policy_loss\", float(\"nan\"))\n",
        "            value = metrics.get(\"value_loss\", float(\"nan\"))\n",
        "            ent = metrics.get(\"entropy\", float(\"nan\"))\n",
        "            kl = metrics.get(\"approx_kl\", float(\"nan\"))\n",
        "            clip = metrics.get(\"clip_frac\", float(\"nan\"))\n",
        "        else:\n",
        "            loss_val = float(metrics)\n",
        "            policy = value = ent = kl = clip = float(\"nan\")\n",
        "\n",
        "        print(\n",
        "            f\"Update {update}/{TOTAL_UPDATES} \"\n",
        "            f\"| loss {loss_val:.3f} \"\n",
        "            f\"| policy {policy:.3f} \"\n",
        "            f\"| value {value:.3f} \"\n",
        "            f\"| ent {ent:.3f} \"\n",
        "            f\"| kl {kl:.4f} \"\n",
        "            f\"| clip {clip:.3f} \"\n",
        "            f\"| avg_ret {avg_ret:.3f}\"\n",
        "        )\n",
        "\n",
        "# 保存模型\n",
        "ppo_path = Path('/kaggle/working/ppo_frozen_mix.pth')\n",
        "torch.save(agent.model.state_dict(), ppo_path)\n",
        "print(\"Saved\", ppo_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 简单评估（可选）\n",
        "对少量局数做 smoke test，防止训练发散。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def play_one(policy_fn, opp_fn, games=20):\n",
        "    wins = 0\n",
        "    for g in range(games):\n",
        "        board = [0]*(ppo_config.ROWS*ppo_config.COLUMNS)\n",
        "        # 轮流先手：偶数局 PPO 先手(标记1)，奇数局对手先手(标记1)，PPO 用标记2\n",
        "        ppo_first = (g % 2 == 0)\n",
        "        ppo_mark = 1 if ppo_first else 2\n",
        "        current = 1\n",
        "        while True:\n",
        "            if current == ppo_mark:\n",
        "                action = policy_fn(board, ppo_mark)\n",
        "            else:\n",
        "                action = opp_fn(board, current)\n",
        "            board = make_move(board, action, current)\n",
        "            done, winner = is_terminal(board)\n",
        "            if done:\n",
        "                if winner == ppo_mark:\n",
        "                    wins += 1\n",
        "                break\n",
        "            current = 3 - current\n",
        "    return wins / games\n",
        "\n",
        "def ppo_policy(board, mark):\n",
        "    state = encode_state(board, mark)\n",
        "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = agent.model(state_t)\n",
        "        valid = get_valid_moves(board)\n",
        "        mask = torch.full_like(logits, float('-inf'))\n",
        "        mask[0, valid] = 0\n",
        "        logits = logits + mask\n",
        "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    return int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "print(\"PPO vs random:\", play_one(ppo_policy, random_policy, games=20))\n",
        "print(\"PPO vs negamax_simple:\", play_one(ppo_policy, lambda b,m: negamax_simple_policy(b,m,depth=4), games=20))\n",
        "print(\"PPO vs negamax_kaggle:\", play_one(ppo_policy, lambda b,m: negamax_kaggle_policy(b,m,depth=4), games=20))\n",
        "if frozen_dqn:\n",
        "    print(\"PPO vs frozen DQN:\", play_one(ppo_policy, frozen_dqn_policy, games=20))\n",
        "if frozen_az:\n",
        "    print(\"PPO vs frozen AlphaZero:\", play_one(ppo_policy, frozen_alphazero_policy, games=20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 打包提交到 Kaggle（生成 agent.py + 权重 zip）\n",
        "- 使用训练好的 `ppo_frozen_mix.pth`\n",
        "- 生成最小可提交的 `agent.py`，一起打包成 zip 上传到竞赛\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 已不再生成 Kaggle 竞赛提交包，直接上传 Hugging Face 模型权重\n",
        "print(\"Skip Kaggle submission packaging; use HF upload cell below.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 上传到 Hugging Face（可选）\n",
        "需提供环境变量 `HUGGINGFACE_HUB_TOKEN`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "# 需先在上方运行 `notebook_login()` 完成认证\n",
        "to_upload = ppo_path  # 直接上传训练得到的 .pth 权重\n",
        "hub_model_id = \"your-username/connectx-ppo-frozen\"  # 替换为你的仓库 ID\n",
        "create_repo(hub_model_id, exist_ok=True)\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=str(to_upload),\n",
        "    path_in_repo=\"ppo_frozen_mix.pth\",\n",
        "    repo_id=hub_model_id,\n",
        ")\n",
        "print(\"Uploaded .pth to HF repo:\", hub_model_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 从 Hugging Face 拉取刚上传的权重并做快速验证\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# 替换为你实际上传的 HF 仓库 ID\n",
        "hf_repo_id = \"your-username/connectx-ppo-frozen\"\n",
        "\n",
        "# 下载权重到本地缓存\n",
        "hf_ckpt = hf_hub_download(repo_id=hf_repo_id, filename=\"ppo_frozen_mix.pth\")\n",
        "print(\"Downloaded from HF:\", hf_ckpt)\n",
        "\n",
        "# 加载权重到新建的 PPOAgent 并做烟雾测试\n",
        "agent_hf = PPOAgent()\n",
        "agent_hf.model.load_state_dict(torch.load(hf_ckpt, map_location=DEVICE))\n",
        "agent_hf.model.eval()\n",
        "\n",
        "\n",
        "def ppo_policy_hf(board, mark):\n",
        "    state = encode_state(board, mark)\n",
        "    state_t = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = agent_hf.model(state_t)\n",
        "        valid = get_valid_moves(board)\n",
        "        mask = torch.full_like(logits, float('-inf'))\n",
        "        mask[0, valid] = 0\n",
        "        logits = logits + mask\n",
        "        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()\n",
        "    return int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "print(\"HF model vs random:\", play_one(ppo_policy_hf, random_policy, games=10))\n",
        "print(\"HF model vs negamax_simple:\", play_one(ppo_policy_hf, lambda b,m: negamax_simple_policy(b,m,depth=4), games=10))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
