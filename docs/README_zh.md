# ğŸ“˜ ConnectX è¯¦ç»†æ–‡æ¡£

æœ¬æ–‡æ¡£æä¾›äº†å…³äº ConnectX åŒæ™ºèƒ½ä½“é¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®ç°ç»†èŠ‚ã€é…ç½®æŒ‡å—å’Œæ•…éšœæ’é™¤ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

- **å¯¹æ¯”èŒƒå¼**: å¯¹æ¯”åŸºäºä»·å€¼ (Rainbow DQN) ä¸åŸºäºç­–ç•¥ (AlphaZero) çš„å¼ºåŒ–å­¦ä¹ ã€‚
- **è¶…äººç±»è¡¨ç°**: è®­ç»ƒå‡ºè¶…è¶Šæ ‡å‡† Minimax/Negamax åŸºå‡†çš„æ™ºèƒ½ä½“ã€‚
- **å¯å¤ç”¨æ¡†æ¶**: ä¸ºæœªæ¥çš„æ£‹ç›˜æ¸¸æˆ RL é¡¹ç›®åˆ›å»ºä¸€ä¸ªæ¨¡å—åŒ–ç³»ç»Ÿã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

é¡¹ç›®éµå¾ªæ¨¡å—åŒ–æ¶æ„ï¼š

```
connectX/
â”œâ”€â”€ ğŸ“‚ agents/               # æ™ºèƒ½ä½“å®ç°
â”‚   â”œâ”€â”€ ğŸ“‚ base/             # å…±äº«å·¥å…· (é…ç½®, å·¥å…·)
â”‚   â”œâ”€â”€ ğŸ“‚ dqn/              # åŸºå‡† DQN å®ç°
â”‚   â”œâ”€â”€ ğŸ“‚ rainbow/          # Rainbow DQN (6 é¡¹æ”¹è¿›)
â”‚   â””â”€â”€ ğŸ“‚ alphazero/        # AlphaZero (MCTS + ResNet)
â”‚
â”œâ”€â”€ ğŸ“‚ evaluation/           # ç»Ÿä¸€è¯„ä¼°æ¡†æ¶
â”‚   â”œâ”€â”€ arena.py             # å¤´å¯¹å¤´æ¯”èµ›å¼•æ“
â”‚   â”œâ”€â”€ benchmark.py         # æ ‡å‡†å¯¹æ‰‹å¥—ä»¶
â”‚   â””â”€â”€ compare.py           # å¯¹æ¯”å’Œå¯è§†åŒ–
â”‚
â”œâ”€â”€ ğŸ“‚ tools/                # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ prepare_submission.py # Kaggle æäº¤æ‰“åŒ…å™¨
â”‚   â””â”€â”€ visualize.py         # è®­ç»ƒå¯è§†åŒ–
â”‚
â”œâ”€â”€ ğŸ“‚ outputs/              # è®­ç»ƒäº§ç‰©
â”‚   â”œâ”€â”€ checkpoints/         # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ logs/                # TensorBoard æ—¥å¿—
â”‚   â”œâ”€â”€ models/              # æœ€ç»ˆæ¨¡å‹
â”‚   â””â”€â”€ plots/               # ç”Ÿæˆçš„å›¾è¡¨
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                 # æ–‡æ¡£
â””â”€â”€ ğŸ“‚ submission/           # Kaggle æäº¤æ–‡ä»¶
```

## ğŸš€ æ‰©å±•å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…

```bash
pip install -r requirements.txt
```

### 2. è¿è¡Œå®Œæ•´å®éªŒ

```bash
# å®Œæ•´è®­ç»ƒæµç¨‹
python run_experiment.py

# å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (å¿«é€ŸéªŒè¯)
python run_experiment.py --quick
```

### 3. è®­ç»ƒå•ä¸ªæ™ºèƒ½ä½“

**Rainbow DQN:**

```bash
python -m agents.rainbow.train_rainbow
```

**AlphaZero:**

```bash
python -m agents.alphazero.train_alphazero
```

### 4. è¯„ä¼°å’Œå¯¹æ¯”

```bash
# è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶
python -m evaluation.benchmark

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
python -m evaluation.compare
```

### 5. å‡†å¤‡ Kaggle æäº¤

```bash
# Rainbow DQN
python tools/prepare_submission.py \
    --agent rainbow \
    --model-path outputs/models/rainbow/best_model.pth

# AlphaZero
python tools/prepare_submission.py \
    --agent alphazero \
    --model-path outputs/models/alphazero/best_model.pth
```

## ğŸ“Š ä¸»è¦ç‰¹æ€§ä¸å®ç°

### ğŸŒˆ Rainbow DQN

Rainbow ç»“åˆäº†åŸå§‹ DQN ç®—æ³•çš„å…­é¡¹æ‰©å±•ï¼š

1.  **Double DQN**: è§£è€¦é€‰æ‹©ä¸è¯„ä¼°ä»¥å‡å°‘é«˜ä¼°åå·®ã€‚
2.  **Prioritized Experience Replay (PER)**: æ›´é¢‘ç¹åœ°é‡‡æ ·é‡è¦çš„è½¬æ¢ã€‚
3.  **Dueling Networks**: ä½¿ç”¨ä¸¤ä¸ªæµï¼ˆä»·å€¼å’Œä¼˜åŠ¿ï¼‰æ¥ä¼°è®¡ Q å€¼ã€‚
4.  **Multi-step Learning**: ä½¿ç”¨ n æ­¥å›æŠ¥æ›´å¿«åœ°ä¼ æ’­å¥–åŠ±ã€‚
5.  **Noisy Nets**: å‘æƒé‡æ·»åŠ å‚æ•°åŒ–å™ªå£°ä»¥è·å¾—æ›´å¥½çš„æ¢ç´¢ã€‚
6.  **Distributional RL (C51)**: å¯¹å›æŠ¥åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œè€Œä¸ä»…ä»…æ˜¯å‡å€¼ï¼ˆå¯é€‰ï¼‰ã€‚

**é…ç½® (`agents/rainbow/rainbow_config.py`):**

```python
LEARNING_RATE = 1e-4
BATCH_SIZE = 256
GAMMA = 0.99
PER_ALPHA = 0.6
N_STEP = 3
```

### ğŸ¤– AlphaZero

AlphaZero ä½¿ç”¨ä¸€ç§å¹¿ä¹‰è¿­ä»£ç®—æ³•ï¼š

1.  **MCTS**: åŸºäºå½“å‰ç­–ç•¥ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢è¿›è¡Œå‰ç»è§„åˆ’ã€‚
2.  **Policy-Value Network**: ä¸€ä¸ªè¾“å‡ºç§»åŠ¨æ¦‚ç‡ ($p$) å’Œä½ç½®ä»·å€¼ ($v$) çš„æ®‹å·®ç½‘ç»œã€‚
3.  **Self-Play**: æ™ºèƒ½ä½“ä¸è‡ªå·±å¯¹å¼ˆä»¥ç”Ÿæˆè®­ç»ƒæ•°æ® $(s, \pi, z)$ã€‚
4.  **Symmetry**: åˆ©ç”¨æ£‹ç›˜çš„æ°´å¹³å¯¹ç§°æ€§ä½¿è®­ç»ƒæ•°æ®åŠ å€ã€‚

**é…ç½® (`agents/alphazero/az_config.py`):**

```python
NUM_SIMULATIONS = 800
C_PUCT = 1.5
LEARNING_RATE = 0.01
NUM_SELFPLAY_GAMES = 500
```

## ğŸ”¬ è¯„ä¼°æ¡†æ¶

### æ ‡å‡†å¯¹æ‰‹

åŸºå‡†æµ‹è¯•å¥—ä»¶é’ˆå¯¹ä»¥ä¸‹å¯¹æ‰‹æµ‹è¯•æ™ºèƒ½ä½“ï¼š

- **Random**: åŸºå‡† (ELO ~800)
- **Negamax (Depth 2)**: å¼±å‰ç» (ELO ~1200)
- **Negamax (Depth 4)**: ä¸­ç­‰å‰ç» (ELO ~1400)
- **Negamax (Depth 6)**: å¼ºå‰ç» (ELO ~1600)

### æŒ‡æ ‡

- **èƒœç‡ (Win Rate)**: èµ¢å¾—æ¯”èµ›çš„ç™¾åˆ†æ¯”ã€‚
- **ELO è¯„åˆ† (ELO Rating)**: ä¼°è®¡çš„ç›¸å¯¹æŠ€èƒ½æ°´å¹³ã€‚
- **å†³ç­–æ—¶é—´ (Decision Time)**: æ¯æ¬¡ç§»åŠ¨çš„å¹³å‡æ—¶é—´ã€‚

## ğŸ“ˆ ç›‘æ§

ä½¿ç”¨ TensorBoard ç›‘æ§è®­ç»ƒè¿›åº¦ï¼š

```bash
tensorboard --logdir outputs/logs
```

**å…³æ³¨æŒ‡æ ‡:**

- **Rainbow**: `loss`, `avg_q_value`, `epsilon` (å¦‚æœä¸æ˜¯ noisy net)ã€‚
- **AlphaZero**: `policy_loss`, `value_loss`, `total_loss`ã€‚

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**é—®é¢˜: è®­ç»ƒå¤ªæ…¢ã€‚**

- **ä¿®å¤**: å‡å° `BATCH_SIZE`ï¼Œä½¿ç”¨ GPUï¼Œæˆ–å‡å°‘ `NUM_SIMULATIONS` (å¯¹äº AlphaZero)ã€‚

**é—®é¢˜: æ™ºèƒ½ä½“ä¸‹å‡ºæ— æ•ˆç§»åŠ¨ã€‚**

- **ä¿®å¤**: ç¡®ä¿åœ¨æ¨¡å‹è¾“å‡ºä¸­æ­£ç¡®åº”ç”¨äº†åŠ¨ä½œæ©ç ã€‚

**é—®é¢˜: Kaggle æäº¤è¶…æ—¶ã€‚**

- **ä¿®å¤**: å¯¹äº AlphaZeroï¼Œå‡å°‘æ¨ç†æ—¶çš„ MCTS æ¨¡æ‹Ÿæ¬¡æ•°ã€‚å¯¹äº Rainbowï¼Œç¡®ä¿æ¨¡å‹ä¸è¦å¤ªæ·±ã€‚

## ğŸ“š å‚è€ƒèµ„æ–™

- [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)
- [Mastering the Game of Go without Human Knowledge (AlphaZero)](https://nature.com/articles/nature24270)
