# ğŸš€ Quick Start Guide

å¿«é€Ÿä¸Šæ‰‹ ConnectX åŒæ™ºèƒ½ä½“é¡¹ç›®

## âš¡ 5åˆ†é’Ÿå¿«é€Ÿæµ‹è¯•

```bash
# 1. ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬è®¾ç½®ç¯å¢ƒï¼ˆæ¨èï¼‰
# Windows:
scripts\setup_env.bat

# Linux/Mac:
chmod +x scripts/setup_env.sh
./scripts/setup_env.sh

# æˆ–æ‰‹åŠ¨å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬ï¼‰
# Windows:
venv\Scripts\activate.bat
# Linux/Mac:
source venv/bin/activate

# 3. å¿«é€Ÿæµ‹è¯•ï¼ˆçº¦5-10åˆ†é’Ÿï¼‰
python run_full_experiment.py --quick

# 4. æŸ¥çœ‹ç»“æœ
ls experiments/comparison_*/comparison_report.html
```

## ğŸ“– è¯¦ç»†æ­¥éª¤

### æ­¥éª¤1ï¼šç¯å¢ƒå‡†å¤‡

#### æ–¹æ³•Aï¼šè‡ªåŠ¨åŒ–è®¾ç½®ï¼ˆæ¨èï¼‰

**Windows:**
```bash
# è¿è¡Œè‡ªåŠ¨åŒ–è„šæœ¬
scripts\setup_env.bat
```

**Linux/Mac:**
```bash
# æ·»åŠ æ‰§è¡Œæƒé™å¹¶è¿è¡Œ
chmod +x scripts/setup_env.sh
./scripts/setup_env.sh
```

è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- æ£€æŸ¥ Python ç‰ˆæœ¬ï¼ˆéœ€è¦ 3.8+ï¼‰
- åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ `venv/`
- å®‰è£…æ‰€æœ‰ä¾èµ–ï¼ˆåŒ…æ‹¬ PyTorch, NumPy, Matplotlib, Pygame, TensorBoard ç­‰ï¼‰

#### æ–¹æ³•Bï¼šæ‰‹åŠ¨è®¾ç½®

```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬ (éœ€è¦3.8+)
python --version

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate.bat
# Linux/Mac:
source venv/bin/activate

# å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt

# å¯é€‰ï¼šCUDAæ”¯æŒï¼ˆGPUåŠ é€Ÿï¼‰
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### æ­¥éª¤2ï¼šé€‰æ‹©è®­ç»ƒæ–¹æ¡ˆ

#### æ–¹æ¡ˆAï¼šå¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆæ¨èæ–°æ‰‹ï¼‰
```bash
# è®­ç»ƒæ—¶é—´ï¼šçº¦10-20åˆ†é’Ÿ
# ç›®çš„ï¼šéªŒè¯ä»£ç æ­£å¸¸å·¥ä½œ
python run_full_experiment.py --quick
```

#### æ–¹æ¡ˆBï¼šä»…è®­ç»ƒRainbow DQN
```bash
# è®­ç»ƒæ—¶é—´ï¼šæ•°å°æ—¶åˆ°1å¤©
cd rainbow
python train_rainbow.py
```

#### æ–¹æ¡ˆCï¼šä»…è®­ç»ƒAlphaZero
```bash
# è®­ç»ƒæ—¶é—´ï¼š1-2å¤©
cd alphazero
python train_alphazero.py
```

#### æ–¹æ¡ˆDï¼šå®Œæ•´è®­ç»ƒï¼ˆéœ€è¦GPUï¼‰
```bash
# è®­ç»ƒæ—¶é—´ï¼š3-7å¤©
python run_full_experiment.py
```

### æ­¥éª¤3ï¼šç›‘æ§è®­ç»ƒè¿›åº¦

```bash
# åœ¨æ–°ç»ˆç«¯ä¸­å¯åŠ¨TensorBoard
tensorboard --logdir rainbow/logs/runs --logdir alphazero/logs/runs

# è®¿é—® http://localhost:6006
```

å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡ï¼š
- **Loss**: åº”è¯¥ä¸‹é™
- **Win Rate**: åº”è¯¥ä¸Šå‡
- **Q Values**: åº”è¯¥è¶‹äºç¨³å®š
- **ELO Rating**: åº”è¯¥å¢é•¿

### æ­¥éª¤4ï¼šè¯„ä¼°æ¨¡å‹

```bash
# è¿è¡ŒåŸºå‡†æµ‹è¯•
python -m evaluation.benchmark

# æˆ–ä½¿ç”¨Pythonè„šæœ¬
python << EOF
from evaluation.benchmark import Benchmark
from agents.rainbow.rainbow_agent import RainbowAgent
from evaluation.arena import create_agent_wrapper

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
agent = RainbowAgent()
agent.load_model('rainbow/checkpoints/best_rainbow_*.pth')

# è¿è¡ŒåŸºå‡†æµ‹è¯•
benchmark = Benchmark()
results = benchmark.run_benchmark(
    create_agent_wrapper(agent, 'rainbow'),
    agent_name="My Rainbow Agent",
    games_per_opponent=50
)
EOF
```

### æ­¥éª¤5ï¼šå‡†å¤‡Kaggleæäº¤

```bash
# Rainbow DQNæäº¤
python tools/prepare_kaggle_submission.py \
    --agent rainbow \
    --model-path rainbow/checkpoints/best_rainbow_full_20251125_*.pth \
    --output submission/my_rainbow_agent.py

# AlphaZeroæäº¤  
python tools/prepare_kaggle_submission.py \
    --agent alphazero \
    --model-path alphazero/checkpoints/best_alphazero_20251125_*.pth \
    --output submission/my_alphazero_agent.py \
    --mcts-sims 100
```

### æ­¥éª¤6ï¼šæœ¬åœ°æµ‹è¯•æäº¤æ–‡ä»¶

```python
# æµ‹è¯•Rainbow agent
from submission.my_rainbow_agent import agent

# æ¨¡æ‹ŸKaggle observation
class Obs:
    def __init__(self):
        self.board = [0] * 42
        self.mark = 1

obs = Obs()
action = agent(obs, None)
print(f"Agent selected action: {action}")
```

## ğŸ® äº¤äº’å¼å¯¹å¼ˆï¼ˆå¯é€‰ï¼‰

```python
from evaluation.arena import Arena
from agents.rainbow.rainbow_agent import RainbowAgent
from evaluation.benchmark import StandardOpponents

# åŠ è½½ä½ çš„agent
my_agent = RainbowAgent()
my_agent.load_model('rainbow/checkpoints/best_rainbow.pth')

# åˆ›å»ºå¯¹æˆ˜åœº
arena = Arena()

# å¯¹æˆ˜æµ‹è¯•
results = arena.play_match(
    agent1_fn=lambda b,m: my_agent.select_action(b, m, epsilon=0),
    agent2_fn=StandardOpponents.negamax_depth_4,
    num_games=10,
    agent1_name="My Agent",
    agent2_name="Negamax-4",
    verbose=True
)
```

## ğŸ“Š æŸ¥çœ‹ç»“æœ

### 1. TensorBoardå¯è§†åŒ–
```bash
tensorboard --logdir experiments/
```

### 2. HTMLæŠ¥å‘Š
æ‰“å¼€æµè§ˆå™¨è®¿é—®:
```
experiments/comparison_*/comparison_report.html
```

### 3. JSONæ•°æ®
```python
import json

with open('experiments/rainbow_benchmark.json') as f:
    data = json.load(f)
    
print(f"Overall win rate: {data['overall']['overall_win_rate']:.1%}")
print(f"Estimated ELO: {data['overall']['estimated_elo']:.0f}")
```

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ
**A**: å‡ ä¸ªè§£å†³æ–¹æ¡ˆï¼š
```bash
# 1. ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
python run_full_experiment.py --quick

# 2. å‡å°‘è®­ç»ƒè½®æ•°
# ç¼–è¾‘ rainbow/rainbow_config.py
SELF_PLAY_EPISODES = 1000  # é»˜è®¤8000

# 3. å‡å°‘MCTSæ¨¡æ‹Ÿæ¬¡æ•°
# ç¼–è¾‘ alphazero/az_config.py
NUM_SIMULATIONS = 200  # é»˜è®¤800
```

### Q: å†…å­˜ä¸è¶³ï¼Ÿ
**A**: å‡å°bufferå¤§å°ï¼š
```python
# rainbow/rainbow_config.py
REPLAY_BUFFER_SIZE = 100000  # é»˜è®¤500000
BATCH_SIZE = 128  # é»˜è®¤256

# alphazero/az_config.py
REPLAY_BUFFER_SIZE = 200000  # é»˜è®¤500000
BATCH_SIZE = 256  # é»˜è®¤512
```

### Q: å¦‚ä½•ä½¿ç”¨GPUï¼Ÿ
**A**: PyTorchä¼šè‡ªåŠ¨æ£€æµ‹GPUï¼š
```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
```

### Q: å¦‚ä½•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Ÿ
**A**: 
```python
from agents.rainbow.rainbow_agent import RainbowAgent

agent = RainbowAgent()
agent.load_model('path/to/model.pth')
# æˆ–
agent.load_checkpoint('path/to/checkpoint.pth')
```

### Q: è®­ç»ƒä¸­æ–­äº†å¦‚ä½•æ¢å¤ï¼Ÿ
**A**: 
```python
# Rainbow
from agents.rainbow.rainbow_agent import RainbowAgent
from rainbow.train_rainbow import RainbowTrainer

agent = RainbowAgent()
agent.load_checkpoint('rainbow/checkpoints/rainbow_ep5000.pth')

trainer = RainbowTrainer(agent)
trainer.train(num_episodes=3000)  # ç»§ç»­è®­ç»ƒ

# AlphaZero
from alphazero.train_alphazero import AlphaZeroTrainer

trainer = AlphaZeroTrainer()
trainer.load_checkpoint('alphazero/checkpoints/alphazero_iter50.pth')
trainer.train(max_iterations=50)  # ç»§ç»­è®­ç»ƒ
```

## ğŸ“ ä¸‹ä¸€æ­¥

å®Œæˆå¿«é€Ÿå¼€å§‹åï¼Œä½ å¯ä»¥ï¼š

1. **è°ƒä¼˜è¶…å‚æ•°**
   - ä¿®æ”¹ `rainbow/rainbow_config.py`
   - ä¿®æ”¹ `alphazero/az_config.py`

2. **å®éªŒä¸åŒæ¶æ„**
   - å°è¯•æ›´æ·±çš„ç½‘ç»œ
   - è°ƒæ•´ResBlockæ•°é‡
   - æµ‹è¯•Distributional RL

3. **æ·»åŠ æ–°å¯¹æ‰‹**
   - å®ç°è‡ªå®šä¹‰ç­–ç•¥
   - æ·»åŠ åˆ°benchmark suite

4. **ä¼˜åŒ–æ€§èƒ½**
   - ä½¿ç”¨æ¨¡å‹é‡åŒ–
   - å®ç°æ‰¹å¤„ç†æ¨ç†
   - å¤šGPUå¹¶è¡Œè®­ç»ƒ

5. **æäº¤åˆ°Kaggle**
   - å‡†å¤‡submissionæ–‡ä»¶
   - æœ¬åœ°æµ‹è¯•
   - ä¸Šä¼ å¹¶è¯„ä¼°

## ğŸ¯ æ¨èå­¦ä¹ è·¯å¾„

### åˆå­¦è€…
1. è¿è¡Œ `--quick` æ¨¡å¼ç†è§£æµç¨‹
2. é˜…è¯» `DUAL_AGENT_README.md`
3. ç ”ç©¶ `rainbow/rainbow_agent.py` ä»£ç 
4. å°è¯•ä¿®æ”¹ç®€å•å‚æ•°é‡æ–°è®­ç»ƒ

### ä¸­çº§ç”¨æˆ·
1. å®Œæ•´è®­ç»ƒRainbow DQN
2. åˆ†æTensorBoardæ—¥å¿—
3. å®ç°è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
4. ä¼˜åŒ–è¶…å‚æ•°

### é«˜çº§ç”¨æˆ·
1. å®Œæ•´è®­ç»ƒä¸¤ä¸ªagent
2. å®ç°åˆ†å¸ƒå¼è®­ç»ƒ
3. æ·»åŠ æ–°çš„RLç®—æ³•
4. å‚ä¸Kaggleç«èµ›

## ğŸ†˜ è·å–å¸®åŠ©

- ğŸ“– å®Œæ•´æ–‡æ¡£: `DUAL_AGENT_README.md`
- ğŸ’¡ å®ç°ç»†èŠ‚: `IMPLEMENTATION_SUMMARY.md`
- ğŸ› é—®é¢˜æŠ¥å‘Š: GitHub Issues
- ğŸ’¬ è®¨è®º: GitHub Discussions

---

å¦‚æœ‰é—®é¢˜éšæ—¶æŸ¥é˜…æ–‡æ¡£æˆ–æissue!

